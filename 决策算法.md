**强化学习：**强化学习是想让一个智能体(agent)在不同的环境状态(state)下，学会选择那个使得奖赏(reward)最大的动作(action)。

一个强化学习系统有以下四个主要的组成元素：

- policy（策略π）  可以看成一个关于状态s的函数F；
- reward（奖励）   选择一系列的动作去最大化总的未来奖励；
- value function（值函数）   表示一个状态s的平均reward；
- environment model（环境模型）。

​			如果我们知道环境的一切，我们就说这个环境是已知的，即model based。

​			但是在现实生活中，我们是很难知道状态之间的转移概率，称为model free。

<img src="https://img-blog.csdnimg.cn/20210320145602494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMzAyMDA0,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

强化学习算法有两个重要的**评价指标**：一个是算法收敛后的策略在初始状态下的期望回报，另一个是样本复杂度，即算法达到收敛结果需要在真实环境中采样的样本数量。







## 1 马尔科夫决策过程（Markov Decision Process, MDP）

### 1.1 马尔可夫决策过程

1. **马尔可夫性**：

下一个状态的产生只和当前的状态有关，而与之前的状态无关。
$$
P[S_{t+1}|S_t]=P[S_{t+1}|S_1,...,S_t]
$$
可以这样理解，St包含了之前全部状态S1，S2，S3，…，St-1的全部信息，只要知道St，之前的历史信息就可以抛弃了。

2. **随机过程**：

而数学中用来描述随机变量序列的方式就是随机过程；

如果这个随机过程中的每个状态都是符合马尔可夫性的，那么则称这个随机过程为马尔可夫随机过程。

3. **马尔可夫过程**：

马尔可夫过程定义为：(S, P);      指具有马尔可夫性的随机过程，也叫**马尔可夫链**

​		其中S是有限状态集，P是状态转移概率（是一个矩阵，描述了S中每一种状态到领一种状态的转移概率）∑P=1

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220902215231662.png" alt="image-20220902215231662" style="zoom: 67%;" />

一组状态序列称为**马尔科夫链**。



4. **马尔科夫奖励过程**：在马尔可夫过程的基础上加入奖励函数r和折扣因子γ，就可以得到马尔可夫奖励过程<S, P, r, γ>
4. **马尔科夫决策过程**：

在马尔可夫过程的基础上加上**动作**和**反馈**就是马尔可夫决策过程。

马尔可夫决策过程定义为：( S , A , P , R , γ ) 
S为有限状态集；A为有限动作集；P为状态转移概率；R为回报函数；γ为折扣因子（用来计算累积回报）；

回报(Return): Ut = Rt + γRt+1 + γ^2 Rt+2 + ……

### 1.2 策略与回报

1. **策略**：

强化学习的目标就是给定一个马尔可夫决策过程，去寻找最优的策略；

策略可以理解为在状态s下选择某一个动作a的概率；

智能体根据当前状态从动作的集合中选择一个动作的函数，被称为策略。

π(a|s)=p(At=a | St = s);    策略π在每个状态s指定一个每个动作a的发生概率；

- 确定性策略：它在每个状态时只输出一个确定性的动作，即只有该动作的概率为 1，其他动作的概率为 0
- 随机性策略：它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。

2. **累计回报**：

给定策略π，从状态s1出发可能产生若干马尔可夫链,如：“s1−s2−s3−s4−s5”，或者“s1−s2−s3−s5 ”，针对某一条确定的马尔可夫链，我们可以计算该链的累计回报：

(Return): Ut = Rt + γRt+1 + γ^2 Rt+2 + ……

策略不同，累计回报也不同。

为了评价某一个状态s的回报价值，我们将状态s的累计回报的期望作为评价指标，称为**值函数**。

3. **值函数**

所有状态的价值就组成了**价值函数**

当智能体针对一个已知的马尔科夫决策过程，采用了策略π，那么将累积回报在状态s处的期望值定义为“**状态价值函数V**”：
$$
V^{\pi} (s) = E_\pi[\sum^\infty_{k=0}\gamma^kR_{t+k+1}|S_t=s]= E_\pi[R_{t+1}+\gamma v(S_{t+1})|S_t = s ]
$$
将累计回报在状态s处采取了行为a的期望定义为“**状态-行为值函数**”即动作价值函数
$$
Q^\pi(s,a) = E_\pi[\sum^\infty_{k=0}\gamma^kR_{t+k+1}|S_t=s,A_t=a]\\= E_\pi[R_{t+1}+\gamma v(S_{t+1})|S_t = s,A_t=a ]
$$

状态价值函数和动作价值函数之间的关系：
$$
V^{\pi}(s)=\sum_{a\in A}\pi(a|s)Q^{\pi}(s,a)\\
Q^{\pi}(s,a)=r(s,a)+\gamma\sum_{s'\in S} P(s'|s,a)V^{\pi}(s')
$$


**贝尔曼方程**：
$$
V(s)=r(s)+\gamma\sum_{s'\in S}p(s'|s)V(s')
$$
贝尔曼方程矩阵形式

![image-20220902221430520](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220902221430520.png)
$$
V=(I-\gamma P)^{-1}R
$$

求解较大规模的马尔可夫奖励过程中的价值函数时，可以使用**动态规划**（dynamic programming）算法、**蒙特卡洛方法**（Monte-Carlo method）和**时序差分**（temporal difference）

**贝尔曼期望方程**：

![image-20220903161542432](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220903161542432.png)

状态价值函数的贝尔曼期望方程：

​		输入：当前状态s、动作集A、状态集S、策略Π、状态转移函数、下一时刻的状态价值、γ		输出：当前状态s的状态价值

动作价值函数的贝尔曼期望方程：

​		输入：reward、γ、动作集A、状态集S、状态转移函数、策略Π、下一时刻的动作价值

**蒙特卡洛方法**

蒙特卡洛方法（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用蒙特卡洛方法时，我们通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。

**占用度量**：它表示动作状态对(s,a)被访问到的概率
$$
\rho^{\pi}(s,a)=(1-\gamma)\sum^{\infty}_{t=0}\gamma^{t}P^{\pi}_t(s)\pi(a|s)
$$
状态访问分布与占用度量的关系：
$$
\rho^{\pi}(s,a)=v^{\pi}(s)\pi(a|s)
$$


![image-20220903164714349](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220903164714349.png)



**最优策略**：有限状态和动作集合的 MDP 中，至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是**最优策略**（optimal policy）。最优策略可能有很多个，我们都将其表示为π*(s)

**最优状态价值函数**
$$
V^*(s)=\max_{\pi	}V^{\pi}(s), ∀s\in S
$$
**最优动作价值函数**
$$
Q^*(s,a)=\max_{\pi}Q^{\pi}(s,a)\ \ ∀s\in S,a\in A
$$
​		两者的关系：![image-20220903171519079](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220903171519079.png)



**贝尔曼最优方程**：

![image-20220903171556181](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220903171556181.png)

## 2 部分可观测的马尔科夫决策过程



## 3 动态规划算法

**动态规划**（dynamic programming）是程序设计算法中非常重要的内容，能够高效解决一些经典问题，例如背包问题和最短路径规划。动态规划的基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到目标问题的解。动态规划会保存已解决的子问题的答案，在求解目标问题的过程中，需要这些子问题答案时就可以直接利用，避免重复计算。

基于动态规划的强化学习算法主要有两种：一是**策略迭代**（policy iteration），二是**价值迭代**（value iteration）。

**策略迭代算法**

策略迭代是策略评估和策略提升不断循环交替，直至最后得到最优策略的过程。
$$
V^{k+1}(s)=\sum_{a\in A}\pi(a|s)(r(s,a)+\gamma\sum_{s'\in S}P(s'|s,a)V^k(s'))\\我们可以选定任意初始值V_0，当k→\infty 时序列{V^k}会收敛到V^{\pi}
$$

当max|Vk+1-Vk|非常小时可以提前结束评估，提高效率

策略提升定理：略

这个根据贪心法选取动作从而得到新的策略的过程称为策略提升。当策略提升之后得到的策略 π‘ 和之前的策略 π 一样时，说明策略迭代达到了收敛，此时和就是最优策略。

  **算法流程：**

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220912203621691.png" alt="image-20220912203621691" style="zoom:50%;" />

**价值迭代算法**：

​		价值迭代可以看成一种动态规划过程。它利用贝尔曼最优方程
$$
V^*(s)=\max_{a\in A}\{r(s,a)+\gamma\sum P(s'|s,a)V^*(s')\}
$$
将其写成迭代更新的方式为：
$$
V^{k+1}(s)=\max_{a\in A}\{r(s,a)+\gamma\sum P(s'|s,a)V^k(s')\}\\等到V^{k+1}和V^{k}相同时，它就是贝尔曼最优方程的不动点
$$
​	**算法流程：**

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220912210548140.png" alt="image-20220912210548140" style="zoom:50%;" />

## 4 时序差分算法

**在线策略学习**要求使用在当前策略下采样得到的样本进行学习，一旦策略被更新，当前的样本就被放弃了。

**离线策略学习**使用经验回放池将之前采样得到的样本收集起来再次利用，离线策略学习往往能够更好地利用历史数据，并具有更小的样本复杂度。

**时序差分**是一种用来估计一个策略的价值函数的方法，它结合了蒙特卡洛和动态规划算法的思想。
$$
V(s_t)←V(s_t)+\alpha[r_t+\gamma V(s_{t+1})-V(s_t)]\\其中R_t+\gamma V(s_{t+1})-V(s_t)被称为时序差分误差(TD \ error)
$$
其中，α为更新步长，是一个常数；

基于时序差分的强化学习算法：Sarsa和Q-learning

**Sarsa算法**：(在线策略算法)

时序差分更新方式：
$$
Q(s_t,a_t)←Q(s_t,a_t)+\alpha(r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t))
$$
采用一个*ε*-贪婪策略：有1-*ε*的概率采用动作价值最大的那个动作，另外有*ε*的概率从动作空间中随机采取一个动作，其公式表示为：
$$
\pi(a|s)= \begin{cases}
      \epsilon/|A|+1-\epsilon,  & 如果a=arg\max_{a'}Q(s,a') \\
      \epsilon/|A|, & \text{其他动作}
    \end{cases}
    
$$
​		算法流程：

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220912214718699.png" alt="image-20220912214718699" style="zoom:50%;" />

**Q-learning算法**：（离线策略算法）

时序差分更新方式：
$$
Q(s_t,a_t)←Q(s_t,a_t)+\alpha(R_t+\gamma\max Q(s_{t+1},a)-Q(s_t,a_t))
$$
算法流程：

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220912222045953.png" alt="image-20220912222045953" style="zoom:50%;" />

Q-learning是直接在估计Q*，因为动作价值函数的贝尔曼最优方程是：
$$
Q^*(s,a)=r(s,a)+\gamma\sum_{s'\in S}P(s'|s,a)\max_{a'}Q^*(s',a')
$$


## 5 Dyna-Q算法

算法流程：

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221012204646954.png" alt="image-20221012204646954" style="zoom:50%;" />

在每次与环境进行交互执行一次 Q-learning 之后，Dyna-Q 会做n次 Q-planning。其中 Q-planning 的N次数是一个事先可以选择的**超参数**，当其为 0 时就是普通的 Q-learning。

## 6 DQN算法

**Deep Q Network(DQN)**：是将神经网略（neural network）和Q-learning结合，利用神经网络近似模拟函数Q（s，a），输入是问题的状态，输出是每个动作a对应的Q值，然后依据Q值大小选择对应状态执行的动作，以完成控制。

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220901165742320.png" alt="image-20220901165742320" style="zoom:50%;" />

将 Q 网络的**损失函数**构造为均方误差的形式：
$$
w^*=arg \min_w \frac{1}{2N} \sum_{i=1}^N[Q_w(s_i,a_i)-(r_i+\gamma\max_{a^{'}}Q_w(s_i^{'},a_i^{'}))]^2
$$
**经验回放**：具体做法为维护 一个回放缓冲区，将每次从环境中采样得到的四元组数据（S、a、R、S'）存储到回放缓冲区中，训练 Q 网络的时候再从回放缓冲区中随机采样若干数据来进行训练。

​	作用：1. 使样本满足独立假设。在 MDP中交互采样得到的数据本身不满足独立假设，因为这一时刻的状态和上一时刻的状态有关。非独立同分布的数据对训练神经网络有很大的影响，会使神经网络拟合到最近训练的数据上。采用经验回放可以打破样本之间的相关性，让其满足独立假设。

2. 提高样本效率。每一个样本可以被使用多次，十分适合深度神经网络的梯度学习。

**目标网络：**既然训练过程中 Q 网络的不断更新会导致目标不断发生改变，不如暂时先将 TD 目标中的 Q 网络固定住。

DQN算法可以用来解决连续状态空间和离散动作空间问题，无法解决连续动作空间类问题。

**算法流程**：

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221012215402703.png" alt="image-20221012215402703" style="zoom: 67%;" />

## 7 Double DQN算法

Double DQN 算法提出利用两个独立训练的神经网络估算。具体做法是将原有的更改为，即利用一套神经网络的输出选取价值最大的动作，但在使用该动作的价值时，用另一套神经网络计算该动作的价值。这样，即使其中一套神经网络的某个动作存在比较严重的过高估计问题，由于另一套神经网络的存在，这个动作最终使用的值不会存在很大的过高估计问题。

Double DQN 算法中的第一套神经网络来选取动作，将目标网络作为第二套神经网络计算Q值，这便是 Double DQN 的主要思想。

## 8 PPO算法



## 9 DDPG算法
